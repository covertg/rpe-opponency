{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code/Variables setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Simulation parameters ##########\n",
    "eta = 0.01          # Learning rate\n",
    "n_trials0 = 1000    # Number of trials in the stationary environment\n",
    "n_trials1 = 1000    # Number of trials after decreasing/increasing the reward probability\n",
    "n_trials = n_trials0 + n_trials1\n",
    "trials0 = list(range(n_trials0))\n",
    "trials = list(range(n_trials))\n",
    "GAMMA = 0.35        # Discount rate, for TD errors\n",
    "REWARD_DELTA = 0.25 # Maximum quantity by which to increase or decrease the reward when it's time for volatility.\n",
    "\n",
    "########## Reproducibility ##########\n",
    "seed = 2021\n",
    "rng = None          # We call `rng = np.random.default_rng(seed=seed)` at the start of each major section.\n",
    "\n",
    "########## Plotting config ##########\n",
    "sns.set_theme(\n",
    "    style='white',\n",
    "    palette='tab10',\n",
    "    font_scale=1.15,\n",
    "    rc={'figure.autolayout': True, 'figure.dpi': 300, 'figure.figsize': (8, 8),\n",
    "        # 'font.family': 'Fira Code', 'font.weight': 'light',\n",
    "        'font.family': 'serif',\n",
    "        'text.latex.preamble': r'\\usepackage{amsmath}', 'text.usetex': True,  # for \\text command\n",
    "        'xtick.bottom': True, 'ytick.left': True\n",
    "    }\n",
    ")\n",
    "HIDE = True\n",
    "def hideplt():      # Helper macro to not embed figures in this notebook when HIDE=True\n",
    "    if HIDE: plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rpes_for_prob(reward_prob, discount_gamma, decrease):\n",
    "    rewards = np.zeros(n_trials, dtype=np.int)\n",
    "    rpes = np.zeros(n_trials)\n",
    "    Q = 0.5  # Sort of a neutral prior, the mean of 0 and 1. Just helps convergence happen slightly more quickly.\n",
    "    for i in trials:\n",
    "        if i == n_trials0:\n",
    "            if decrease:\n",
    "                reward_prob = max(0, reward_prob - REWARD_DELTA)\n",
    "            else:\n",
    "                reward_prob = min(1, reward_prob + REWARD_DELTA)\n",
    "        rewards[i] = int(rng.random() < reward_prob)\n",
    "        rpes[i] = rewards[i] + discount_gamma * Q - Q\n",
    "        Q += eta * rpes[i]\n",
    "    return rpes, rewards\n",
    "\n",
    "def calc_rpes_for_probs(reward_probs, discount_gamma, decrease):\n",
    "    outputs = [calc_rpes_for_prob(p, discount_gamma=discount_gamma, decrease=decrease) for p in reward_probs]\n",
    "    rpes = [o[0] for o in outputs]\n",
    "    rewards = [o[1] for o in outputs]\n",
    "    return rpes, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do RPEs do?\n",
    "\n",
    "Initial, exploratory foray into the (asymptotic) properties of RPEs in a stationary environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "probs = np.linspace(1, 0, num=9)\n",
    "rpe_RW, reward_RW = calc_rpes_for_probs(probs, discount_gamma=0, decrease=True)\n",
    "rpe_TD, reward_TD = calc_rpes_for_probs(probs, discount_gamma=GAMMA, decrease=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rpe_asymptote(rpes, type):\n",
    "    f, axs = plt.subplots(3, 3, sharex=True, sharey=True)\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, (rpe, p) in enumerate(zip(rpes, probs)):\n",
    "        sns.scatterplot(x=trials0, y=rpe[:n_trials0], linewidth=0, s=8, ax=axs[i])\n",
    "        if p > 0:\n",
    "            pos_line = 1 - p\n",
    "            axs[i].axhline(y=pos_line, color='green', alpha=0.5, linestyle='dotted')\n",
    "        if p < 1:\n",
    "            neg_line = -1 * p\n",
    "            axs[i].axhline(y=neg_line, color='red', alpha=0.5, linestyle='dotted')\n",
    "        axs[i].set_title(f'$p={p}$')\n",
    "        axs[i].set_xlabel('Trial')\n",
    "        axs[i].set_ylabel('RPE')\n",
    "        axs[i].set_ylim([-1, 1])\n",
    "\n",
    "    f.suptitle('Simulated ' + type + ' RPEs under the optimal policy\\n' +\n",
    "               '\\\\normalsize{Stationary environments with varying reward probability $p$}',\n",
    "               x=0, y=1, horizontalalignment='left', verticalalignment='top')\n",
    "    legend_stuff = [\n",
    "        matplotlib.lines.Line2D([0], [0], color='green', alpha=0.5, linestyle='dotted', label='Positive Limit, $y=1-p$'),\n",
    "        matplotlib.lines.Line2D([0], [0], color='red', alpha=0.5, linestyle='dotted', label='Negative Limit, $y=-p$')\n",
    "    ]\n",
    "    f.legend(handles=legend_stuff, loc='upper right', bbox_to_anchor=(1, 0.9835))\n",
    "    return f\n",
    "\n",
    "f = plot_rpe_asymptote(rpe_RW, 'RW')\n",
    "f.savefig('integrators/rpe-asmpytote-rw.pdf')\n",
    "hideplt()\n",
    "\n",
    "f = plot_rpe_asymptote(rpe_TD, 'TD')\n",
    "f.savefig('integrators/rpe-asmptote-td.pdf')\n",
    "hideplt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rpe_histogram(rpes, name):\n",
    "\n",
    "    f, axs = plt.subplots(3, 3, sharex=True, sharey=True)\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, (rpe, p) in enumerate(zip(rpes, probs)):\n",
    "        positives = np.sum(rpe[:n_trials0] > 0) / n_trials0\n",
    "        negatives = np.sum(rpe[:n_trials0] < 0) / n_trials0\n",
    "        sns.barplot(x=['Positive', 'Negative'], y=[positives, negatives], palette=['green', 'red'], ax=axs[i])\n",
    "        axs[i].axhline(y=p, color='green', alpha=0.5, linestyle='dotted')\n",
    "        axs[i].axhline(y=1-p, color='red', alpha=0.5, linestyle='dotted')\n",
    "        axs[i].set_title(f'$p={p}$')\n",
    "        if i in [0, 3, 6]:\n",
    "            axs[i].set_ylabel(f'Proportion over {n_trials0} trials')\n",
    "        if i in [6, 7, 8]:\n",
    "            axs[i].set_xlabel('RPE Sign')\n",
    "\n",
    "    f.suptitle('Density histogram of ' + name + ' RPE sign under the optimal policy\\n' +\n",
    "               '\\\\large{Stationary environments with varying reward probability $p$}',\n",
    "               x=0, y=1, horizontalalignment='left', verticalalignment='top')\n",
    "    legend_stuff = [\n",
    "        matplotlib.lines.Line2D([0], [0], color='green', alpha=0.5, linestyle='dotted', label='Reward prob, $p$'),\n",
    "        matplotlib.lines.Line2D([0], [0], color='red', alpha=0.5, linestyle='dotted', label='Non-reward prob, $1-p$')\n",
    "    ]\n",
    "    f.legend(handles=legend_stuff, loc='upper right', bbox_to_anchor=(1, 0.9835))\n",
    "\n",
    "    return f\n",
    "\n",
    "f = plot_rpe_histogram(rpe_RW, 'RW')\n",
    "f.savefig('integrators/rpesign-RW.pdf')\n",
    "hideplt()\n",
    "\n",
    "f = plot_rpe_histogram(rpe_TD, 'TD')\n",
    "f.savefig('integrators/rpesign-TD.pdf')\n",
    "hideplt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Dives into Reward Integration\n",
    "\n",
    "Remainder of the notebook. Two key figures to produce:\n",
    "\n",
    "* Figure of Surprise against reward probability, for each integrator\n",
    "* \"Ridgelines\" figure of smoothed surprise against trial, across various reward probabilities, for each integrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rint_double_neg(rpes):\n",
    "    xis_f = util.shift_reward_integration(orig_weight_past=0.99, orig_weight_pres=1.5, past_shift=-0.009)\n",
    "    xis_s = util.shift_reward_integration(orig_weight_past=0.99, orig_weight_pres=1.5, past_shift=0.002)\n",
    "    dbar_f, dbar_s = np.zeros(n_trials), np.zeros(n_trials)\n",
    "    for i, rpe in enumerate(rpes):\n",
    "        prev_dbar_f = 0 if i == 0 else dbar_f[i - 1]\n",
    "        prev_dbar_s = 0 if i == 0 else dbar_s[i - 1]\n",
    "        dbar_f[i] = xis_f['past'] * prev_dbar_f + xis_f['pres'] * min(0, rpe)\n",
    "        dbar_s[i] = xis_s['past'] * prev_dbar_s + xis_s['pres'] * min(0, rpe)\n",
    "    dbar = np.minimum(0, dbar_f - dbar_s)\n",
    "    return dbar\n",
    "\n",
    "def rint_single_neg(rpes):\n",
    "    xis = util.shift_reward_integration(orig_weight_past=0.99, orig_weight_pres=1.5, past_shift=0)\n",
    "    dbar = np.zeros(n_trials)\n",
    "    for i, rpe in enumerate(rpes):\n",
    "        prev_dbar = 0 if i == 0 else dbar[i - 1]\n",
    "        dbar[i] = xis['past'] * prev_dbar + xis['pres'] * min(0, rpe)\n",
    "    return dbar\n",
    "\n",
    "def rint_double_posneg(rpes):\n",
    "    xis_f = util.shift_reward_integration(orig_weight_past=0.99, orig_weight_pres=1.5, past_shift=-0.009)\n",
    "    xis_s = util.shift_reward_integration(orig_weight_past=0.99, orig_weight_pres=1.5, past_shift=0.002)\n",
    "    dbar_f_pos, dbar_s_pos = np.zeros(n_trials), np.zeros(n_trials)\n",
    "    dbar_f_neg, dbar_s_neg = np.zeros(n_trials), np.zeros(n_trials)\n",
    "    for i, rpe in enumerate(rpes):\n",
    "        prev_dbar_f_pos = 0 if i == 0 else dbar_f_pos[i - 1]\n",
    "        prev_dbar_s_pos = 0 if i == 0 else dbar_s_pos[i - 1]\n",
    "        prev_dbar_f_neg = 0 if i == 0 else dbar_f_neg[i - 1]\n",
    "        prev_dbar_s_neg = 0 if i == 0 else dbar_s_neg[i - 1]\n",
    "        dbar_f_pos[i] = xis_f['past'] * prev_dbar_f_pos + xis_f['pres'] * max(0, rpe)\n",
    "        dbar_s_pos[i] = xis_s['past'] * prev_dbar_s_pos + xis_s['pres'] * max(0, rpe)\n",
    "        dbar_f_neg[i] = xis_f['past'] * prev_dbar_f_neg + xis_f['pres'] * min(0, rpe)\n",
    "        dbar_s_neg[i] = xis_s['past'] * prev_dbar_s_neg + xis_s['pres'] * min(0, rpe)\n",
    "    dbar_pos = np.maximum(0, dbar_f_pos - dbar_s_pos)\n",
    "    dbar_neg = np.minimum(0, dbar_f_neg - dbar_s_neg)\n",
    "    dbar = np.absolute(dbar_pos + dbar_neg)\n",
    "    return dbar\n",
    "\n",
    "def rint_single_posneg(rpes):\n",
    "    xis_pos = util.shift_reward_integration(orig_weight_past=0.99, orig_weight_pres=1.5, past_shift=0)\n",
    "    xis_neg = util.shift_reward_integration(orig_weight_past=0.99, orig_weight_pres=1.5, past_shift=0)\n",
    "    dbar_pos, dbar_neg = np.zeros(n_trials), np.zeros(n_trials)\n",
    "    for i, rpe in enumerate(rpes):\n",
    "        prev_dbar_pos = 0 if i == 0 else dbar_pos[i - 1]\n",
    "        prev_dbar_neg = 0 if i == 0 else dbar_neg[i - 1]\n",
    "        dbar_pos[i] = xis_pos['past'] * prev_dbar_pos + xis_pos['pres'] * max(0, rpe)\n",
    "        dbar_neg[i] = xis_neg['past'] * prev_dbar_neg + xis_neg['pres'] * min(0, rpe)\n",
    "    dbar = np.absolute(dbar_pos + dbar_neg)\n",
    "    return dbar\n",
    "\n",
    "def rint_daw(rpes, rewards, reward_lr=0.05):\n",
    "    dbar, rbar = np.zeros(n_trials), np.zeros(n_trials)\n",
    "    for i, (rpe, reward) in enumerate(zip(rpes, rewards)):\n",
    "        prev_rbar = 0.5 if i == 0 else rbar[i - 1]\n",
    "        rbar[i] = reward_lr * reward + (1 - reward_lr) * prev_rbar\n",
    "        dbar[i] = rpe - rbar[i]\n",
    "    return dbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=seed)\n",
    "probs = np.linspace(0, 1, num=101)\n",
    "\n",
    "# RPEs for experiments where the reward probability decreases\n",
    "RW = np.array([calc_rpes_for_probs(probs, discount_gamma=0, decrease=True) for _ in range(30)])\n",
    "rpes_RW, rewards_RW = RW[:, 0], RW[:, 1]\n",
    "TD = np.array([calc_rpes_for_probs(probs, discount_gamma=GAMMA, decrease=True) for _ in range(30)])\n",
    "rpes_TD, rewards_TD = TD[:, 0], TD[:, 1]\n",
    "\n",
    "# RPEs for experiments where the reward probability increases\n",
    "RW_inc = np.array([calc_rpes_for_probs(probs, discount_gamma=0, decrease=False) for _ in range(30)])\n",
    "rpes_RW_inc, rewards_RW_inc = RW_inc[:, 0], RW_inc[:, 1]\n",
    "TD_inc = np.array([calc_rpes_for_probs(probs, discount_gamma=GAMMA, decrease=False) for _ in range(30)])\n",
    "rpes_TD_inc, rewards_TD_inc = TD_inc[:, 0], TD_inc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RW \"effective dbars\"\n",
    "rint_d_n_all = np.array([[rint_double_neg(rpe) for rpe in rpes] for rpes in rpes_RW])  # e.g. shape = (runs, probs, trials)\n",
    "rint_s_n_all = np.array([[rint_single_neg(rpe) for rpe in rpes] for rpes in rpes_RW])\n",
    "rint_d_pn_all = np.array([[rint_double_posneg(rpe) for rpe in rpes] for rpes in rpes_RW])\n",
    "rint_s_pn_all = np.array([[rint_single_posneg(rpe) for rpe in rpes] for rpes in rpes_RW])\n",
    "rint_daw_all = np.array([[rint_daw(rpe, reward) for rpe, reward in zip(rpes, rewards)] for rpes, rewards in zip(rpes_RW, rewards_RW)])\n",
    "\n",
    "######################################################################################################################################\n",
    "# TD \"effective dbars\"\n",
    "\n",
    "rint_d_n_all_td = np.array([[rint_double_neg(rpe) for rpe in rpes] for rpes in rpes_TD])  # e.g. shape = (runs, probs, trials)\n",
    "rint_s_n_all_td = np.array([[rint_single_neg(rpe) for rpe in rpes] for rpes in rpes_TD])\n",
    "rint_d_pn_all_td = np.array([[rint_double_posneg(rpe) for rpe in rpes] for rpes in rpes_TD])\n",
    "rint_s_pn_all_td = np.array([[rint_single_posneg(rpe) for rpe in rpes] for rpes in rpes_TD])\n",
    "rint_daw_all_td = np.array([[rint_daw(rpe, reward) for rpe, reward in zip(rpes, rewards)] for rpes, rewards in zip(rpes_TD, rewards_TD)])\n",
    "\n",
    "######################################################################################################################################\n",
    "# RW \"effective dbars\" where the reward prob increases rather than decreases\n",
    "\n",
    "rint_d_n_all_inc = np.array([[rint_double_neg(rpe) for rpe in rpes] for rpes in rpes_RW_inc])  # e.g. shape = (runs, probs, trials)\n",
    "rint_s_n_all_inc = np.array([[rint_single_neg(rpe) for rpe in rpes] for rpes in rpes_RW_inc])\n",
    "rint_d_pn_all_inc = np.array([[rint_double_posneg(rpe) for rpe in rpes] for rpes in rpes_RW_inc])\n",
    "rint_s_pn_all_inc = np.array([[rint_single_posneg(rpe) for rpe in rpes] for rpes in rpes_RW_inc])\n",
    "rint_daw_all_inc = np.array([[rint_daw(rpe, reward) for rpe, reward in zip(rpes, rewards)] for rpes, rewards in zip(rpes_RW_inc, rewards_RW_inc)])\n",
    "\n",
    "######################################################################################################################################\n",
    "# TD \"effective dbars\" where the reward prob increases rather than decreases\n",
    "\n",
    "rint_d_n_all_inc_td = np.array([[rint_double_neg(rpe) for rpe in rpes] for rpes in rpes_TD_inc])  # e.g. shape = (runs, probs, trials)\n",
    "rint_s_n_all_inc_td = np.array([[rint_single_neg(rpe) for rpe in rpes] for rpes in rpes_TD_inc])\n",
    "rint_d_pn_all_inc_td = np.array([[rint_double_posneg(rpe) for rpe in rpes] for rpes in rpes_TD_inc])\n",
    "rint_s_pn_all_inc_td = np.array([[rint_single_posneg(rpe) for rpe in rpes] for rpes in rpes_TD_inc])\n",
    "rint_daw_all_inc_td = np.array([[rint_daw(rpe, reward) for rpe, reward in zip(rpes, rewards)] for rpes, rewards in zip(rpes_TD_inc, rewards_TD_inc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the average dbar (effective, unscaled) for each reward integration mechanism,\n",
    "# for each calculation of prediction error,\n",
    "# in stationary environments across all reward probs.\n",
    "\n",
    "def plot_rint_smooth(rint_all, name, type):\n",
    "    # Maintain reward_prob and trial axes; smooth across \"run\" axis\n",
    "    rint_smooth = np.mean(rint_all, axis=0)\n",
    "    # Calculate mean in the last 100 trials of the stationary block\n",
    "    means_by_rewardprob = np.mean([rint[n_trials0-100:n_trials0] for rint in rint_smooth], axis=-1)\n",
    "    fg = sns.relplot(x=probs, y=means_by_rewardprob, linewidth=0, s=8, kind='scatter')\n",
    "    fg.axes[0, 0].set_xlabel('Reward Probability')\n",
    "    fg.axes[0, 0].set_ylabel(r'$\\text{mean}(\\text{effective }\\bar \\delta \\text{, last 100 trials})$')\n",
    "    fg.fig.suptitle('Mean of effective $\\\\bar \\\\delta$ upon value convergence with ' + type + ' RPE\\n' +\n",
    "                    '\\\\normalsize{' + name + ' integrator, optimal policy, stationary environment}\\n' +\n",
    "                    '\\\\normalsize{Smoothed across 30 runs}',\n",
    "                    x=0, y=1, horizontalalignment='left', verticalalignment='top')\n",
    "    return fg\n",
    "\n",
    "plot_rint_smooth(rint_d_n_all, 'Two-Timescale Negative', 'RW').savefig('integrators/converged-mean-doubleneg.pdf')\n",
    "hideplt()\n",
    "plot_rint_smooth(rint_s_n_all, 'One-Timescale Negative', 'RW').savefig('integrators/converged-mean-singleneg.pdf')\n",
    "hideplt()\n",
    "plot_rint_smooth(rint_d_pn_all, 'Two-Timescale Pos/Neg', 'RW').savefig('integrators/converged-mean-doubleposneg.pdf')\n",
    "hideplt()\n",
    "plot_rint_smooth(rint_s_pn_all, 'One-Timescale Pos/Neg', 'RW').savefig('integrators/converged-mean-singleposneg.pdf')\n",
    "hideplt()\n",
    "plot_rint_smooth(rint_daw_all, 'Daw et al. DA/5HT', 'RW').savefig('integrators/converged-mean-daw.pdf')\n",
    "hideplt()\n",
    "\n",
    "######################################################################################################################################\n",
    "\n",
    "plot_rint_smooth(rint_d_n_all_td, 'Two-Timescale Negative', 'TD').savefig('integrators/converged-mean-doubleneg-TD.pdf')\n",
    "hideplt()\n",
    "plot_rint_smooth(rint_s_n_all_td, 'One-Timescale Negative', 'TD').savefig('integrators/converged-mean-singleneg-TD.pdf')\n",
    "hideplt()\n",
    "plot_rint_smooth(rint_d_pn_all_td, 'Two-Timescale Pos/Neg', 'TD').savefig('integrators/converged-mean-doubleposneg-TD.pdf')\n",
    "hideplt()\n",
    "plot_rint_smooth(rint_s_pn_all_td, 'One-Timescale Pos/Neg', 'TD').savefig('integrators/converged-mean-singleposneg-TD.pdf')\n",
    "hideplt()\n",
    "plot_rint_smooth(rint_daw_all_td, 'Daw et al. DA/5HT', 'TD').savefig('integrators/converged-mean-daw-TD.pdf')\n",
    "hideplt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RW\n",
      "[-9.80224027 -3.07024636]\n",
      "[-28.58822694 -10.01080247]\n",
      "[1.36095501 6.24649194]\n",
      "[ 4.90294435 16.69165636]\n",
      "[-0.79165542 -0.20932016]\n",
      "\n",
      "TD\n",
      "[-8.08310697 -2.95642527]\n",
      "[-23.38970872  -7.32741487]\n",
      "[1.61127077 7.76406534]\n",
      "[ 6.02918143 23.17503487]\n",
      "[-0.68944206 -0.14724725]\n"
     ]
    }
   ],
   "source": [
    "# Heuristic to estimate the \"window\" of effective dbar to consider: 25% and 75% quantiles.\n",
    "def quantile_rint_all(rint_all, q=[0.25, 0.75]):\n",
    "    x = np.array([rint[:, 0:250] for rint in rint_all]).flatten()\n",
    "    return np.quantile(x, q)\n",
    "\n",
    "q_d_n = quantile_rint_all(rint_d_n_all)\n",
    "q_s_n = quantile_rint_all(rint_s_n_all)\n",
    "q_d_pn = quantile_rint_all(rint_d_pn_all)\n",
    "q_s_pn = quantile_rint_all(rint_s_pn_all)\n",
    "q_daw = quantile_rint_all(rint_daw_all)\n",
    "print('RW', q_d_n, q_s_n, q_d_pn, q_s_pn, q_daw, sep='\\n')\n",
    "\n",
    "q_d_n_td = quantile_rint_all(rint_d_n_all_td)\n",
    "q_s_n_td = quantile_rint_all(rint_s_n_all_td)\n",
    "q_d_pn_td = quantile_rint_all(rint_d_pn_all_td)\n",
    "q_s_pn_td = quantile_rint_all(rint_s_pn_all_td)\n",
    "q_daw_td = quantile_rint_all(rint_daw_all_td)\n",
    "print('\\nTD', q_d_n_td, q_s_n_td, q_d_pn_td, q_s_pn_td, q_daw_td, sep='\\n')\n",
    "\n",
    "# pd.Series(rint_daw_all.flatten()).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort of linear sigmoid function. Transforms and scales the inputs to values between 1 and 0.\n",
    "def window_rint(rint, bound0, bound1):\n",
    "    adjusted = (rint - bound0) / (bound1 - bound0)\n",
    "    return np.clip(adjusted, 0, 1)\n",
    "\n",
    "######################################################################################################################################\n",
    "# RW \"scaled dbar\" variables.\n",
    "\n",
    "# Calculate 30 runs of reward integration for each mechanism. These will be used to smooth the ridgeline plots.\n",
    "rint_d_n_all_scale = np.array([window_rint(rint, q_d_n[1], q_d_n[0]) for rint in rint_d_n_all])\n",
    "rint_s_n_all_scale = np.array([window_rint(rint, q_s_n[1], q_s_n[0]) for rint in rint_s_n_all])\n",
    "rint_d_pn_all_scale = np.array([window_rint(rint, q_d_pn[0], q_d_pn[1]) for rint in rint_d_pn_all])\n",
    "rint_s_pn_all_scale = np.array([window_rint(rint, q_s_pn[0], q_s_pn[1]) for rint in rint_s_pn_all])  # todo ! fix.\n",
    "rint_daw_all_scale = np.array([window_rint(rint, 1, -2) for rint in rint_daw_all])  # NOTE not q_daw[1], q_daw[0]\n",
    "\n",
    "rint_d_n_all_scale_inc = np.array([window_rint(rint, q_d_n[1], q_d_n[0]) for rint in rint_d_n_all_inc])\n",
    "rint_s_n_all_scale_inc = np.array([window_rint(rint, q_s_n[1], q_s_n[0]) for rint in rint_s_n_all_inc])\n",
    "rint_d_pn_all_scale_inc = np.array([window_rint(rint, q_d_pn[0], q_d_pn[1]) for rint in rint_d_pn_all_inc])\n",
    "rint_s_pn_all_scale_inc = np.array([window_rint(rint, q_s_pn[0], q_s_pn[1]) for rint in rint_s_pn_all_inc])\n",
    "rint_daw_all_scale_inc = np.array([window_rint(rint, 1, -2) for rint in rint_daw_all_inc])  # NOTE not q_daw[1], q_daw[0]\n",
    "\n",
    "# print(pd.Series(rint_daw_0_scale.flatten()).describe())\n",
    "# print(pd.Series(window_rint(rint_daw_all[0], 1, -2).flatten()).describe())\n",
    "\n",
    "######################################################################################################################################\n",
    "# TD \"scaled dbar\" variables.\n",
    "\n",
    "# Calculate 30 runs of reward integration for each mechanism. These will be used to smooth the ridgeline plots.\n",
    "rint_d_n_all_scale_td = np.array([window_rint(rint, q_d_n_td[1], q_d_n_td[0]) for rint in rint_d_n_all_td])\n",
    "rint_s_n_all_scale_td = np.array([window_rint(rint, q_s_n_td[1], q_s_n_td[0]) for rint in rint_s_n_all_td])\n",
    "rint_d_pn_all_scale_td = np.array([window_rint(rint, q_d_pn_td[0], q_d_pn_td[1]) for rint in rint_d_pn_all_td])\n",
    "rint_s_pn_all_scale_td = np.array([window_rint(rint, q_s_pn_td[0], q_s_pn_td[1]) for rint in rint_s_pn_all_td])  # todo ! fix.\n",
    "rint_daw_all_scale_td = np.array([window_rint(rint, 1, -2) for rint in rint_daw_all_td])  # NOTE not q_daw[1], q_daw[0]\n",
    "\n",
    "rint_d_n_all_scale_inc_td = np.array([window_rint(rint, q_d_n_td[1], q_d_n_td[0]) for rint in rint_d_n_all_inc_td])\n",
    "rint_s_n_all_scale_inc_td = np.array([window_rint(rint, q_s_n_td[1], q_s_n_td[0]) for rint in rint_s_n_all_inc_td])\n",
    "rint_d_pn_all_scale_inc_td = np.array([window_rint(rint, q_d_pn_td[0], q_d_pn_td[1]) for rint in rint_d_pn_all_inc_td])\n",
    "rint_s_pn_all_scale_inc_td = np.array([window_rint(rint, q_s_pn_td[0], q_s_pn_td[1]) for rint in rint_s_pn_all_inc_td])\n",
    "rint_daw_all_scale_inc_td = np.array([window_rint(rint, 1, -2) for rint in rint_daw_all_inc_td])  # NOTE not q_daw[1], q_daw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_surprise_1state(dbscale):\n",
    "    # Agent \"attention\"\n",
    "    I_SC = 0.5\n",
    "    w_A = (1 - dbscale) * I_SC + dbscale\n",
    "    # Cue prototypes\n",
    "    mu, sigma = 1.0, 0.05\n",
    "    cue = 1.2  # Changing this seems to only change the linear scaling of the y-axis. Neato.\n",
    "    # Activation and surprise\n",
    "    A = scipy.stats.norm.pdf(x=cue*w_A, loc=mu*w_A, scale=sigma)\n",
    "    A = np.maximum(A, util.TINYNONZERO) \n",
    "    F = -1 * np.log(A)\n",
    "    return F\n",
    "\n",
    "def calc_surprise_2state(dbscale):\n",
    "    # Agent \"attention\"\n",
    "    I_SC = np.array([0.99, 0.01])\n",
    "    w_As = [[((1 - d) * I_SC[0] + d, (1 - d) * I_SC[1] + d) for d in dbscale_p] for dbscale_p in dbscale]\n",
    "    w_As = np.array(w_As)  # shape (100, 1000, 2) = (reward_probs, trials, n_cues)\n",
    "    # Cue prototypes\n",
    "    mu = np.array([1.0, 0.5])\n",
    "    proto0 = np.array([1.0, 0])\n",
    "    proto1 = np.array([1.0, 1])\n",
    "    cues0 = proto0 + rng.normal(loc=0, scale=0.05, size=(100000, 2))\n",
    "    cues1 = proto1 + rng.normal(loc=0, scale=0.05, size=(100000, 2))\n",
    "    cues = np.concatenate([cues0, cues1])\n",
    "    Sigma = np.cov(cues, rowvar=False)\n",
    "    cue = np.array([1.1, -0.1])\n",
    "    # Activation and surprise\n",
    "    A = np.zeros_like(dbscale)\n",
    "    for p in range(len(w_As)):\n",
    "        for t in range(len(w_As[p])):\n",
    "            A[p, t] = scipy.stats.multivariate_normal.pdf(x=w_As[p, t] * cue, mean=w_As[p, t] * mu, cov=Sigma)\n",
    "    A = np.maximum(A, util.TINYNONZERO)\n",
    "    F = -1 * np.log(A)\n",
    "    return F\n",
    "\n",
    "######################################################################################################################################\n",
    "# RW surprise variables.\n",
    "\n",
    "surp_d_n_all = np.array([calc_surprise_1state(rint_scale) for rint_scale in rint_d_n_all_scale])\n",
    "surp_s_n_all = np.array([calc_surprise_1state(rint_scale) for rint_scale in rint_s_n_all_scale])\n",
    "surp_d_pn_all = np.array([calc_surprise_1state(rint_scale) for rint_scale in rint_d_pn_all_scale])\n",
    "surp_s_pn_all = np.array([calc_surprise_1state(rint_scale) for rint_scale in rint_s_pn_all_scale])\n",
    "surp_daw_all = np.array([calc_surprise_1state(rint_scale) for rint_scale in rint_daw_all_scale])\n",
    "\n",
    "surp_d_n_all_inc = np.array([calc_surprise_1state(rint_scale) for rint_scale in rint_d_n_all_scale_inc])\n",
    "surp_s_n_all_inc = np.array([calc_surprise_1state(rint_scale) for rint_scale in rint_s_n_all_scale_inc])\n",
    "surp_d_pn_all_inc = np.array([calc_surprise_1state(rint_scale) for rint_scale in rint_d_pn_all_scale_inc])\n",
    "surp_s_pn_all_inc = np.array([calc_surprise_1state(rint_scale) for rint_scale in rint_s_pn_all_scale_inc])\n",
    "surp_daw_all_inc = np.array([calc_surprise_1state(rint_scale) for rint_scale in rint_daw_all_scale_inc])\n",
    "\n",
    "# Just pick the first run from each for our main figure, showing surprise by reward prob for each mechanism.\n",
    "surp_d_n = surp_d_n_all[0]\n",
    "surp_s_n = surp_s_n_all[0]\n",
    "surp_d_pn = surp_d_pn_all[0]\n",
    "surp_s_pn = surp_s_pn_all[0]\n",
    "surp_daw = surp_daw_all[0]\n",
    "\n",
    "######################################################################################################################################\n",
    "# TD surprise variables.\n",
    "\n",
    "surp_d_n_all_td = np.array([calc_surprise_1state(rint_scale) for rint_scale in rint_d_n_all_scale_td])\n",
    "surp_s_n_all_td = np.array([calc_surprise_1state(rint_scale) for rint_scale in rint_s_n_all_scale_td])\n",
    "surp_d_pn_all_td = np.array([calc_surprise_1state(rint_scale) for rint_scale in rint_d_pn_all_scale_td])\n",
    "surp_s_pn_all_td = np.array([calc_surprise_1state(rint_scale) for rint_scale in rint_s_pn_all_scale_td])\n",
    "surp_daw_all_td = np.array([calc_surprise_1state(rint_scale) for rint_scale in rint_daw_all_scale_td])\n",
    "\n",
    "surp_d_n_all_inc_td = np.array([calc_surprise_1state(rint_scale) for rint_scale in rint_d_n_all_scale_inc_td])\n",
    "surp_s_n_all_inc_td = np.array([calc_surprise_1state(rint_scale) for rint_scale in rint_s_n_all_scale_inc_td])\n",
    "surp_d_pn_all_inc_td = np.array([calc_surprise_1state(rint_scale) for rint_scale in rint_d_pn_all_scale_inc_td])\n",
    "surp_s_pn_all_inc_td = np.array([calc_surprise_1state(rint_scale) for rint_scale in rint_s_pn_all_scale_inc_td])\n",
    "surp_daw_all_inc_td = np.array([calc_surprise_1state(rint_scale) for rint_scale in rint_daw_all_scale_inc_td])\n",
    "\n",
    "# Just pick the first run from each for our main figure, showing surprise by reward prob for each mechanism.\n",
    "surp_d_n_td = surp_d_n_all_td[0]\n",
    "surp_s_n_td = surp_s_n_all_td[0]\n",
    "surp_d_pn_td = surp_d_pn_all_td[0]\n",
    "surp_s_pn_td = surp_s_pn_all_td[0]\n",
    "surp_daw_td = surp_daw_all_td[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.color_palette('tab10')\n",
    "\n",
    "def plot_summary_subplots(vals, names, t0, t1, valname, ylab, color_pt):\n",
    "    colors = sns.color_palette('tab10')\n",
    "    f, axs = plt.subplots(3, 2, sharex=True, sharey=True)\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, (val, name) in enumerate(zip(vals, names)):\n",
    "        val_last100 = val[:, t0:t1]\n",
    "        IQRs = np.quantile(val_last100, q=[0.25, 0.75], axis=-1)\n",
    "        axs[i].fill_between(x=probs, y1=IQRs[0], y2=IQRs[1], alpha=0.2, color=colors[0])\n",
    "        sns.scatterplot(x=probs, y=np.median(val_last100, axis=-1), linewidth=0, s=8, color=colors[color_pt], ax=axs[i])\n",
    "        axs[i].set_title(name)\n",
    "        axs[i].set_xlabel('Reward Probability')\n",
    "        axs[i].set_ylabel(ylab)\n",
    "\n",
    "    axs[-1].set_frame_on(False)\n",
    "    axs[-1].get_yaxis().set_visible(False)\n",
    "    xmin, xmax = axs[-1].get_xaxis().get_view_interval()\n",
    "    ymin, _ = axs[-1].get_yaxis().get_view_interval()\n",
    "    axs[-1].add_artist(matplotlib.lines.Line2D((xmin, xmax), (ymin, ymin), color='black', linewidth=2))\n",
    "    axs[-1].set_xlabel('Reward Probability')\n",
    "\n",
    "    legend_stuff = [\n",
    "        matplotlib.lines.Line2D([0], [0], color=colors[color_pt], linestyle='dotted', label='Median'),\n",
    "        matplotlib.patches.Patch(color=colors[0], alpha=0.2, label='Interquartile Range')\n",
    "    ]\n",
    "    f.legend(handles=legend_stuff, loc='upper right', bbox_to_anchor=(1, 0.9835))\n",
    "    f.suptitle(valname + '\\n' +\n",
    "           '\\\\normalsize{By reward probability $p$, per-mechanism}',\n",
    "           x=0, y=1, horizontalalignment='left', verticalalignment='top')\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rint_d_n_0_scale = rint_d_n_all_scale[0]\n",
    "rint_s_n_0_scale = rint_s_n_all_scale[0]\n",
    "rint_d_pn_0_scale = rint_d_pn_all_scale[0]\n",
    "rint_s_pn_0_scale = rint_s_pn_all_scale[0]\n",
    "rint_daw_0_scale = rint_daw_all_scale[0]\n",
    "\n",
    "scaled = [rint_d_n_0_scale, rint_s_n_0_scale, rint_d_pn_0_scale, rint_s_pn_0_scale, rint_daw_0_scale]\n",
    "names = ['Two-Timescale Negative', 'One-Timescale Negative', 'Two-Timescale Pos/Neg', 'One-Timescale Pos/Neg', 'Daw et al. DA/5HT']\n",
    "f = plot_summary_subplots(scaled, names, t0=n_trials0-100, t1=n_trials0,\n",
    "    valname='$\\\\bar \\delta_{\\\\text{scaled}}$ upon value convergence in stationary environment, RW RPE',\n",
    "    ylab=r'$\\bar \\delta_{\\text{scaled}}$ last 100 trials', color_pt=1)\n",
    "f.savefig('integrators/summary_dbscaled_stationary-RW.pdf')\n",
    "hideplt()\n",
    "\n",
    "rint_d_n_0_scale_td = rint_d_n_all_scale_td[0]\n",
    "rint_s_n_0_scale_td = rint_s_n_all_scale_td[0]\n",
    "rint_d_pn_0_scale_td = rint_d_pn_all_scale_td[0]\n",
    "rint_s_pn_0_scale_td = rint_s_pn_all_scale_td[0]\n",
    "rint_daw_0_scale_td = rint_daw_all_scale_td[0]\n",
    "\n",
    "scaled = [rint_d_n_0_scale_td, rint_s_n_0_scale_td, rint_d_pn_0_scale_td, rint_s_pn_0_scale_td, rint_daw_0_scale_td]\n",
    "names = ['Two-Timescale Negative', 'One-Timescale Negative', 'Two-Timescale Pos/Neg', 'One-Timescale Pos/Neg', 'Daw et al. DA/5HT']\n",
    "f = plot_summary_subplots(scaled, names, t0=n_trials0-100, t1=n_trials0,\n",
    "    valname='$\\\\bar \\delta_{\\\\text{scaled}}$ upon value convergence in stationary environment, TD RPE',\n",
    "    ylab=r'$\\bar \\delta_{\\text{scaled}}$ last 100 trials', color_pt=1)\n",
    "f.savefig('integrators/summary_dbscaled_stationary-TD.pdf')\n",
    "hideplt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "surps = [surp_d_n, surp_s_n, surp_d_pn, surp_s_pn, surp_daw]\n",
    "names = ['Two-Timescale Negative', 'One-Timescale Negative', 'Two-Timescale Pos/Neg', 'One-Timescale Pos/Neg', 'Daw et al. DA/5HT']\n",
    "f = plot_summary_subplots(surps, names, t0=n_trials0-100, t1=n_trials0,\n",
    "    valname='Surprise upon value convergence in stationary environment, RW RPE',\n",
    "    ylab='Surprise last 100 trials', color_pt=1)\n",
    "f.savefig('integrators/summary_surprise_1d_stationary-RW.pdf')\n",
    "hideplt()\n",
    "\n",
    "surps = [surp_d_n_td, surp_s_n_td, surp_d_pn_td, surp_s_pn_td, surp_daw_td]\n",
    "names = ['Two-Timescale Negative', 'One-Timescale Negative', 'Two-Timescale Pos/Neg', 'One-Timescale Pos/Neg', 'Daw et al. DA/5HT']\n",
    "f = plot_summary_subplots(surps, names, t0=n_trials0-100, t1=n_trials0,\n",
    "    valname='Surprise upon value convergence in stationary environment, TD RPE',\n",
    "    ylab='Surprise last 100 trials', color_pt=1)\n",
    "f.savefig('integrators/summary_surprise_1d_stationary-TD.pdf')\n",
    "hideplt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surp2_d_n = calc_surprise_2state(rint_d_n_0_scale)\n",
    "# surp2_s_n = calc_surprise_2state(rint_s_n_0_scale)\n",
    "\n",
    "# surp2_d_pn = calc_surprise_2state(rint_d_pn_0_scale)\n",
    "# surp2_s_pn = calc_surprise_2state(rint_s_pn_0_scale)\n",
    "\n",
    "# surp2_daw = calc_surprise_2state(rint_daw_0_scale)\n",
    "\n",
    "# surps = [surp2_d_n, surp2_s_n, surp2_d_pn, surp2_s_pn, surp2_daw]\n",
    "# names = ['Two-Timescale Negative', 'One-Timescale Negative', 'Two-Timescale Pos/Neg', 'One-Timescale Pos/Neg', 'Daw et al. DA/5HT']\n",
    "# f = plot_summary_subplots(surps, names, t0=n_trials0-100, t1=n_trials0,\n",
    "#     valname='Surprise upon value convergence in stationary environment',\n",
    "#     ylab='Surprise last 100 trials', color_pt=1)\n",
    "# f.savefig('integrators/summary_surprise_2d_stationary.pdf')\n",
    "\n",
    "# f = plot_summary_subplots(surps, names, t0=n_trials0, t1=n_trials0+200,\n",
    "#     valname='Surprise after reward volatility',\n",
    "#     ylab='Surprise in trials after $p$ change', color_pt=3)\n",
    "# f.savefig('integrators/summary_surprise_2d_volatile.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ['#d2e3f0', '#f6d3d4', '#d4ecd4']  # Corresponds to a few colors in the 'tab10' palette at 20% transparency\n",
    "\n",
    "def surp_ridgelines(surp_all, ax, color_i):\n",
    "    'Plots the Surprise ridgeline figure into a matplotlib axis object.'\n",
    "    # Plotting setup\n",
    "    overlap = 0.25\n",
    "    idx = np.arange(0, 101, 10)\n",
    "    prob_labels = np.round(probs[idx], decimals=2)\n",
    "    # Calculate x, y values\n",
    "    xrange = np.array(trials)\n",
    "    surp_smooth = np.mean(surp_all, axis=0)\n",
    "    curves = surp_smooth[idx]\n",
    "    curves -= np.min(curves)  # Rebase each surprise curve to be between 0 and 1, in a comparable fashion\n",
    "    curves /= np.max(curves)\n",
    "    offsets = np.array(range(11)) * (1 - overlap)  # y-value offset for each new curve\n",
    "    curves += offsets.reshape(11, 1)\n",
    "    # Ridelines! And fills.\n",
    "    for i, (curve, offset) in enumerate(zip(curves, offsets)):\n",
    "        ax.fill_between(x=xrange[:n_trials0], y1=curve[:n_trials0], y2=offset, color=c[0])\n",
    "        ax.fill_between(x=xrange[n_trials0:], y1=curve[n_trials0:], y2=offset, color=c[color_i])\n",
    "        # Seaborn makes things a little slower, but it plays nicely with automatically hiding redundant features in our subplots\n",
    "        sns.lineplot(x=xrange, y=curve, color='black', alpha=0.8, linewidth=0.5, ax=ax)\n",
    "    # Offset tickmarks by just above the \"0\" mark for each ridge\n",
    "    ax.set_yticks(offsets + 0.1)\n",
    "    ax.set_yticklabels(prob_labels)\n",
    "\n",
    "def plot_ridgeline_subplots(vals, names, decrease, kind):\n",
    "    f, axs = plt.subplots(3, 2, sharex=True, sharey=True)\n",
    "    axs = axs.flatten()\n",
    "    color_i = 1 if decrease else 2\n",
    "    for i, (val, name) in enumerate(zip(vals, names)):\n",
    "        surp_ridgelines(val, axs[i], color_i)\n",
    "        axs[i].set_title(name)\n",
    "        axs[i].set_xlabel('Trial')\n",
    "        axs[i].set_ylabel('Initial $p$')\n",
    "\n",
    "    axs[-1].set_frame_on(False)\n",
    "    axs[-1].get_yaxis().set_visible(False)\n",
    "    xmin, xmax = axs[-1].get_xaxis().get_view_interval()\n",
    "    ymin, _ = axs[-1].get_yaxis().get_view_interval()\n",
    "    axs[-1].add_artist(matplotlib.lines.Line2D((xmin, xmax), (ymin, ymin), color='black', linewidth=2))\n",
    "    axs[-1].set_xlabel('Trial')\n",
    "\n",
    "    legend_stuff = [\n",
    "        matplotlib.patches.Patch(facecolor=c[0], edgecolor='black', linewidth=0.5,\n",
    "                                label='Surprise in stationary environment'),\n",
    "        matplotlib.patches.Patch(facecolor=c[color_i], edgecolor='black', linewidth=0.5,\n",
    "                                label='Surprise after ' + ('decrease' if decrease else 'increase') + ' in $p$')\n",
    "    ]\n",
    "    f.legend(handles=legend_stuff, loc='upper right', bbox_to_anchor=(1, 0.9835))\n",
    "    f.suptitle('Surprise in reward-volatile environment with ' + ('decreasing' if decrease else 'increasing') + ' rewards, ' + kind + ' RPE\\n'\n",
    "           '\\\\normalsize{By reward probability $p$, per-mechanism}\\n' +\n",
    "           '\\\\normalsize{Smoothed across 30 runs}',\n",
    "           x=0, y=1, horizontalalignment='left', verticalalignment='top')\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Two-Timescale Negative', 'One-Timescale Negative', 'Two-Timescale Pos/Neg', 'One-Timescale Pos/Neg', 'Daw et al. DA/5HT']\n",
    "\n",
    "surps_all = [surp_d_n_all, surp_s_n_all, surp_d_pn_all, surp_s_pn_all, surp_daw_all]\n",
    "f = plot_ridgeline_subplots(surps_all, names, decrease=True, kind='RW')\n",
    "f.savefig('integrators/ridgelines-surprise-dec-RW.pdf')\n",
    "hideplt()\n",
    "\n",
    "surps_all = [surp_d_n_all_inc, surp_s_n_all_inc, surp_d_pn_all_inc, surp_s_pn_all_inc, surp_daw_all_inc]\n",
    "f = plot_ridgeline_subplots(surps_all, names, decrease=False, kind='RW')\n",
    "f.savefig('integrators/ridgelines-surprise-inc-RW.pdf')\n",
    "hideplt()\n",
    "\n",
    "surps_all = [surp_d_n_all_td, surp_s_n_all_td, surp_d_pn_all_td, surp_s_pn_all_td, surp_daw_all_td]\n",
    "f = plot_ridgeline_subplots(surps_all, names, decrease=True, kind='TD')\n",
    "f.savefig('integrators/ridgelines-surprise-dec-TD.pdf')\n",
    "hideplt()\n",
    "\n",
    "surps_all = [surp_d_n_all_inc_td, surp_s_n_all_inc_td, surp_d_pn_all_inc_td, surp_s_pn_all_inc_td, surp_daw_all_inc_td]\n",
    "f = plot_ridgeline_subplots(surps_all, names, decrease=False, kind='TD')\n",
    "f.savefig('integrators/ridgelines-surprise-inc-TD.pdf')\n",
    "hideplt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session Load/Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "dill.dump_session('integrators/session.pkl')\n",
    "# dill.load_session('integrators/session.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
